{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0h3SEKvvgHc"
      },
      "outputs": [],
      "source": [
        "1.  What is a Support Vector Machine (SVM), and how does it work?\n",
        "  -> A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression.\n",
        "# Here's a breakdown of how SVMs work:\n",
        "\n",
        "# 1. Finding the Optimal Hyperplane:\n",
        "    *  In a multi-dimensional space, a hyperplane acts as a decision boundary.\n",
        "    *  SVM aims to find the hyperplane that best separates the different classes of data points with the widest possible margin.\n",
        "    *  The margin is the distance between the hyperplane and the closest data points (support vectors).\n",
        "    *  Maximizing the margin helps in better generalization of the model, meaning it performs well on unseen data.\n",
        "# 2. Support Vectors:\n",
        "    *  These are the data points that lie closest to the hyperplane and influence its position.\n",
        "    *  SVM focuses on these critical points to define the decision boundary, ignoring other data points.\n",
        "# 3. Handling Non-linear Data:\n",
        "     *  For data that is not linearly separable (cannot be separated by a straight line or hyperplane),\n",
        "       SVM uses kernel functions to map the data into a higher-dimensional space where it becomes linearly separable.\n",
        "    *  Common kernel functions include polynomial kernels, radial basis function (RBF) kernels, and sigmoid kernels.\n",
        "\n",
        "    # 4. Types of SVM:\n",
        "Linear SVM:\n",
        "   *  Used for linearly separable data.\n",
        "Non-linear SVM:\n",
        "     Uses kernel functions to handle non-linearly separable data.\n",
        "Support Vector Regression (SVR): Used for regression problems, predicting continuous values rather than discrete classes.\n",
        "# 5. Applications:\n",
        "Classification:\n",
        "      Spam detection, image classification, fraud detection.\n",
        "Regression:\n",
        "     Predicting house prices, stock prices.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.   Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "    #  ->  Hard Margin SVM:\n",
        "\n",
        "# Assumes linear separability:\n",
        "      Hard margin SVMs can only be applied to datasets where a hyperplane can perfectly divide the data into classes without any errors.\n",
        "# Maximizes margin:\n",
        "      It aims to find the hyperplane that maximizes the distance (margin) between the hyperplane and the closest data points of each class.\n",
        "# Sensitive to outliers:\n",
        "     If even a single data point is misclassified, the hard margin SVM will fail to find a suitable hyperplane.\n",
        "# Not robust to noise:\n",
        "     It struggles with datasets that have outliers or data points that are not linearly separable.\n",
        "     This video explains how hard margin SVM works and how it can fail with non-linearly separable data\n",
        "\n",
        "\n",
        "# Soft Margin SVM:\n",
        "\n",
        "# Handles non-linear separability:\n",
        "     Soft margin SVMs are designed to work with datasets that are not perfectly linearly separable.\n",
        "# Introduces slack variables:\n",
        "     It allows for some misclassifications by introducing \"slack variables\" which penalize misclassified data points in the optimization process.\n",
        "# Balances margin maximization and error minimization:\n",
        "     Soft margin SVMs find a balance between maximizing the margin and minimizing the number of misclassifications.\n",
        "# More robust to outliers:\n",
        "     By allowing some misclassifications, soft margin SVMs are more robust to outliers and noisy data compared to hard margin SVMs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bGGIFYiHx4vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.  What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "   ->  The Kernel Trick is a mathematical technique used in SVM to handle non-linearly separable data.\n",
        "    * The Kernel Trick in Support Vector Machines (SVMs) is a technique that allows SVMs to find non-linear\n",
        "      decision boundaries in the original feature space by implicitly mapping the data into a higher-dimensional\n",
        "       feature space where it becomes linearly separable.\n",
        "\n",
        "  # EXAMPLE:\n",
        "       One common example of a kernel function is the Radial Basis Function (RBF) kernel, also known as the Gaussian kernel.\n",
        "    # Its formula is:\n",
        "\n",
        "    K(x_i, x_j) = exp(-gamma * ||x_i - x_j||^2)\n",
        "# where:\n",
        "   *  x_i and x_j are two data points.\n",
        "   *  ||x_i - x_j||^2 is the squared Euclidean distance between x_i and x_j.\n",
        "   *  gamma is a hyperparameter that controls the influence of individual training samples.\n",
        "\n",
        "\n",
        "# Use Case:\n",
        "The RBF kernel is particularly useful in situations where the data is not linearly separable and exhibits a complex,\n",
        " non-linear relationship between features and classes. For instance, consider a dataset where data points of one class\n",
        "  are clustered in the center, while data points of another class form a ring around this cluster. In the original 2D space,\n",
        "  a linear boundary cannot separate these classes. The RBF kernel implicitly maps this data into a higher-dimensional space\n",
        "  where a linear hyperplane can effectively separate the central\n",
        " cluster from the surrounding ring, resulting in a circular or non-linear decision boundary in the original 2D space.\n"
      ],
      "metadata": {
        "id": "z8p-nZMsy2dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.  What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "   -> A Naive Bayes classifier is a simple probabilistic classifier that applies Bayes\n",
        "     ' theorem with strong independence assumptions between the features\n",
        "# Bayes' Theorem:\n",
        "   The core of the classifier is based on Bayes' theorem, which calculates the probability of a hypothesis (in this case, a class label) given some observed evidence (the features).\n",
        "  #  Independence Assumption:\n",
        "   The \"naive\" part comes from the assumption that the features are independent of each other, meaning the presence or absence of one feature doesn't affect the probability of another feature, given the class label.\n",
        "# Why \"Naive\"?\n",
        "   This assumption is often not true in real-world data, where features are often correlated.\n",
        "   However, despite this simplification,\n",
        "   Naive Bayes classifiers can be surprisingly effective, especially in text classification and other domains with high dimensionality.\n",
        "# Example:\n",
        "Imagine classifying emails as spam or not spam. A Naive Bayes classifier might consider features\n",
        " like the presence of certain words (e.g., \"discount,\" \"free\") and the sender's address. It would assume that the presence of\n",
        "  \"discount\" in an email is independent of the sender's address, which might not always be the case, but the classifier still works well.\n",
        "# Benefits:\n",
        "    Naive Bayes classifiers are computationally efficient, easy to implement, and perform well with high-dimensional data.\n",
        "   They are also robust to irrelevant features, meaning they can still perform reasonably well even if some features are not informative"
      ],
      "metadata": {
        "id": "dES8UPvj0WSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.  Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "   -> Gaussian Naive Bayes is used for continuous data assuming a normal distribution, Multinomial Naive Bayes handles discrete data with counts\n",
        "       (like word frequencies), and Bernoulli Naive Bayes is appropriate for binary or boolean features representing the presence or absence of an attribute.\n",
        "\n",
        "# Gaussian Naive Bayes:\n",
        "\n",
        "# Data Type:\n",
        "   Continuous, numerical data that follows a normal (Gaussian) distribution.\n",
        "# Example:\n",
        "   Predicting flower species based on petal length and width, where these measurements are assumed to be normally distributed.\n",
        "# How it works:\n",
        "   It assumes that each feature within a class is normally distributed and uses the Gaussian Probability Density Function (PDF)\n",
        "  to calculate the likelihood of a data point belonging to a specific class.\n",
        "# When to use:\n",
        "   When dealing with data where features are continuous and approximately normally distributed, such as sensor readings,\n",
        "    test scores, or financial data.\n",
        "\n",
        "# Multinomial Naive Bayes:\n",
        "\n",
        "# Data Type:\n",
        "    Discrete data with counts or frequencies, like word counts in text documents.\n",
        "# Example:\n",
        "    Spam filtering, where the frequency of certain words (like \"free,\" \"discount,\" etc.) is a strong indicator of spam.\n",
        "# How it works:\n",
        "    It uses the Multinomial Probability Mass Function (PMF) to calculate the probability of observing a specific set of feature\n",
        "   counts (word frequencies) given a class (e.g., spam or not spam).\n",
        "\n"
      ],
      "metadata": {
        "id": "desINOVe0Wf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.  Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train an SVM Classifier with a linear kernel\n",
        "# C is the regularization parameter. A smaller C means more regularization.\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 3. Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "# Support vectors are the data points closest to the hyperplane.\n",
        "# svm_model.support_vectors_ contains the coordinates of the support vectors.\n",
        "print(\"\\nSupport Vectors:\")\n",
        "for sv in svm_model.support_vectors_:\n",
        "    print(sv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2uFnw5G0Why",
        "outputId": "fa5bc419-41c8-4c80-8812-5a0bf6605888"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Support Vectors:\n",
            "[4.8 3.4 1.9 0.2]\n",
            "[5.1 3.3 1.7 0.5]\n",
            "[4.5 2.3 1.3 0.3]\n",
            "[5.6 3.  4.5 1.5]\n",
            "[5.4 3.  4.5 1.5]\n",
            "[6.7 3.  5.  1.7]\n",
            "[5.9 3.2 4.8 1.8]\n",
            "[5.1 2.5 3.  1.1]\n",
            "[6.  2.7 5.1 1.6]\n",
            "[6.3 2.5 4.9 1.5]\n",
            "[6.1 2.9 4.7 1.4]\n",
            "[6.5 2.8 4.6 1.5]\n",
            "[6.9 3.1 4.9 1.5]\n",
            "[6.3 2.3 4.4 1.3]\n",
            "[6.3 2.8 5.1 1.5]\n",
            "[6.3 2.7 4.9 1.8]\n",
            "[6.  3.  4.8 1.8]\n",
            "[6.  2.2 5.  1.5]\n",
            "[6.2 2.8 4.8 1.8]\n",
            "[6.5 3.  5.2 2. ]\n",
            "[7.2 3.  5.8 1.6]\n",
            "[5.6 2.8 4.9 2. ]\n",
            "[5.9 3.  5.1 1.8]\n",
            "[4.9 2.5 4.5 1.7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7.  Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Gaussian Naïve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-WpK-O50Wk3",
        "outputId": "6dcfe760-d5a5-4a43-cab5-facbbb8ac198"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8. Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # RBF kernel is commonly used with C and gamma\n",
        "}\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator (model with best hyperparameters)\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on Test Set:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71B98fs00Wnu",
        "outputId": "d5b86f27-b756-47ca-b5f1-f164024a09e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on Test Set: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9.  Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Load dataset (using a binary classification subset for ROC-AUC)\n",
        "categories = ['alt.atheism', 'sci.space']  # two categories for binary classification\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Convert text to numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train Naïve Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)[:, 1]  # probability for class 1\n",
        "\n",
        "# Step 6: Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC-AUC Score: {:.4f}\".format(roc_auc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fyODa1f37ZF",
        "outputId": "4e2896d1-3274-4ac6-9215-82d12364f8cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10.: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "# Data Preprocessing:\n",
        "# Text Cleaning:\n",
        "Lowercase conversion: Convert all text to lowercase for consistent comparison.\n",
        "Punctuation removal: Remove punctuation marks that might not be meaningful for classification (e.g., commas, periods, exclamation points).\n",
        "Stop word removal: Eliminate common words like \"the,\" \"and,\" \"a\" that don't add much meaning to the classification task.\n",
        "Stemming/Lemmatization: Normalize words to their root form to reduce variations (e.g., \"running,\" \"runs,\" and \"ran\" would become \"run\").\n",
        "# Text Vectorization:\n",
        "Bag-of-Words (BoW): Represent each email as a vector where each element corresponds to a word in the vocabulary, and the value represents the word count in the email.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF): Weights words based on their frequency in the document and overall corpus, giving more importance to rare but relevant words.\n",
        "N-grams: Consider sequences of n words (unigrams, bigrams, trigrams) to capture context.\n",
        "# Handling Missing Data:\n",
        "Imputation:\n",
        "     Fill missing values with a predefined value (e.g., \"unknown\" for text fields, average value for numerical features).\n",
        "Dropping Missing Values:\n",
        "     If the percentage of missing values is high in a specific feature, consider dropping that feature.\n",
        "# Model Selection:\n",
        "Naïve Bayes:\n",
        "Advantages:\n",
        "      Simple, efficient, performs well with high-dimensional data like text, handles sparse data well, and works well with class imbalance.\n",
        "Disadvantages:\n",
        "     Assumes independence of features, which might not be entirely true in real-world data.\n",
        "# Support Vector Machines (SVM):\n",
        "Advantages:\n",
        "    Effective for classification tasks, especially with non-linear data, can handle high-dimensional data, and robust to outliers.\n",
        "Disadvantages:\n",
        "   Can be computationally expensive for large datasets, requires parameter tuning.\n",
        "# Addressing Class Imbalance:\n",
        "Oversampling:\n",
        "     Replicate minority class examples to balance the dataset.\n",
        "# Undersampling:\n",
        "     Randomly remove examples from the majority class.\n",
        "# Cost-sensitive learning:\n",
        "    Assign higher weights to misclassified instances of the minority class.\n",
        "# Evaluation Metrics:\n",
        "Accuracy:\n",
        "     Overall proportion of correctly classified emails.\n",
        "Precision:\n",
        "     Proportion of predicted spam emails that are actually spam.\n",
        "Recall:\n",
        "      Proportion of actual spam emails that are correctly predicted as spam.\n",
        "F1-Score:\n",
        "    Harmonic mean of precision and recall, providing a balanced measure of performance.\n",
        "ROC-AUC (Receiver Operating Characteristic - Area Under Curve): Measures the model's ability to distinguish between spam and non-spam emails at different threshold levels.\n",
        "# Business Impact:\n",
        "Reduced Spam:\n",
        "    Implementing a spam filter significantly reduces the number of unwanted emails received by users, improving user experience and productivity.\n",
        "# Improved Security:\n",
        "    Spam emails can contain phishing attempts or malicious links, so a robust spam filter protects users from security threats.\n",
        "# Cost Savings:\n",
        "      By filtering out spam, companies can save on storage and processing resources."
      ],
      "metadata": {
        "id": "LINLlDAK4VBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}